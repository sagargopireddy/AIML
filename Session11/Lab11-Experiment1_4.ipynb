{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations Of AIML\n",
    "## Session 11\n",
    "### Experiment 1.3: Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is to tune the optimizer - Adam and observe the changes in the output.\n",
    "\n",
    "We will use CIFAR100 dataset.This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each.There are 50,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Adam Optimizer **\n",
    "\n",
    "Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.We compute the decaying averages of past and past squared gradients mt and vt respectively as follows:\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/anoverviewofgradientdescentoptimizationalgorithms-170414055411/95/an-overview-of-gradient-descent-optimization-algorithms-64-638.jpg?cb=1492149859)\n",
    "mt and vt are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As mt and vt are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. β1 and β2 are close to 1).\n",
    "\n",
    "\n",
    "Default values of 0.9 for β1, 0.999 for β2, and 10−8 for ϵ. They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "# Importing config.py file\n",
    "import config as cf\n",
    "from utils import *\n",
    "## Importing python packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for GPU instance\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#Intilizaing the accuracy value as zero\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Phase 1] : Data Preparation')\n",
    "\n",
    "##dataset\n",
    "dataset = 'cifar100'\n",
    "\n",
    "# Preparing the dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cf.mean[dataset], cf.std[dataset]),\n",
    "]) # meanstd transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the standard mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cf.mean[dataset], cf.std[dataset]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and Loading the dataset\n",
    "\n",
    "The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes in the dataset\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downloading the dataset\n",
    "trainset = torchvision.datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR100(root='../data', train=False, download=False, transform=transform_test)\n",
    "### Loading the dataset \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=8)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us build a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "        \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, init_mode='xavier'):  ### supports 'zero', 'normal', 'xavier', 'he' inits\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, num_classes)\n",
    "        \n",
    "        if init_mode == 'zero':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    m.weight.data.zero_()   ### fill tensor elements with zeros\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "        \n",
    "        if init_mode == 'normal':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    m.weight.data.normal_()   ### fill tensor elements with random numbers from normal distribution\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_()\n",
    "        \n",
    "        if init_mode == 'xavier':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    fan_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                    n = fan_in + fan_out\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, math.sqrt(2. / n))\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    size = m.weight.size()\n",
    "                    fan_out = size[0] # number of rows\n",
    "                    fan_in = size[1] # number of columns\n",
    "                    variance = math.sqrt(2.0/(fan_in+fan_out))\n",
    "                    m.weight.data.normal_(0.0, variance)\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, variance)\n",
    "        \n",
    "        if init_mode == 'he':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, math.sqrt(2. / n))\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    size = m.weight.size()\n",
    "                    fan_out = size[0] # number of rows\n",
    "                    fan_in = size[1] # number of columns\n",
    "                    variance = math.sqrt(2.0/(fan_in))\n",
    "                    m.weight.data.normal_(0.0, variance)\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, variance)\n",
    "                \n",
    "                \n",
    "    ## Forward Pass\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Xavier init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling the model\n",
    "net = LeNet(num_classes, init_mode='xavier')\n",
    "# Checking for GPU instance\n",
    "if use_cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intiliazing the loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    # Declaring the values\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Looping over Train data\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Checking for GPU instance\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # Optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Converting inputs and targets into pytorch variables \n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs)\n",
    "        # Storing the outputs size\n",
    "        size_ = outputs.size()\n",
    "        # Reducing the outputs dimenssion\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        # Calculating the loss\n",
    "        loss = criterion(outputs_, targets)\n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        # Optimizer Steps\n",
    "        optimizer.step()\n",
    "        # Calculating the train data\n",
    "        train_loss += loss.data[0]\n",
    "        # Predicting the values\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        # Storing the targets size\n",
    "        total += targets.size(0)\n",
    "        # Calculating the correct values\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        # Printing the data\n",
    "        if batch_idx%30 == 0 or batch_idx == len(trainloader)-1:\n",
    "            # Printing the progress bar\n",
    "            progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    # Storing the epoch,loss and accuracy into a file\n",
    "    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    # Declaring the values\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Looping over the test data\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # Checking for GPU instance\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # Converting inputs and targets into pytorch variables \n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs)\n",
    "        # Storing the outputs size\n",
    "        size_ = outputs.size()\n",
    "        # Reducing the dimenssion\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        # Calculating the loss\n",
    "        loss = criterion(outputs_, targets)\n",
    "        # Storing the sum of loss \n",
    "        test_loss += loss.data[0]\n",
    "        # Storing the predicted values\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        # Storing the targets size\n",
    "        total += targets.size(0)\n",
    "        # Calcualting the correct values\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        # Printing the data\n",
    "        if batch_idx%30 == 0 or batch_idx == len(testloader)-1:\n",
    "            # Printing the progress bar\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    # Printing the validation loss\n",
    "    print('val_loss: ',  test_loss/len(testloader), 'accuracy: ', 100.0*correct/total)\n",
    "    # Storing epoch,loss and accuracy into a file\n",
    "    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    # Checking for best accuracy\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        # Checking whether its a directory or not\n",
    "        if not os.path.isdir('../checkpoint'):\n",
    "            # Creating a directory\n",
    "            os.mkdir('../checkpoint')\n",
    "        # Saving the data\n",
    "        torch.save(state, '../checkpoint_ckpt.t7')\n",
    "        # Storing the accuracy\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'adam_solver'\n",
    "# Creating files in write mode\n",
    "train_loss_file = open(\"../Lab11-Experiment1/\"+experiment+\"train_loss.txt\", \"w\")\n",
    "val_loss_file = open(\"../Lab11-Experiment1/\"+experiment+\"val_loss.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training and Testing the model for 60 epochs\n",
    "for epoch in range(0, 60):\n",
    "    if epoch == 50:\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "    if epoch == 30:\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "    if epoch == 0:\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    # Training the model\n",
    "    train(epoch)\n",
    "    # Testing the model\n",
    "    test(epoch)\n",
    "# Closing the files\n",
    "train_loss_file.close()\n",
    "val_loss_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_curves(\"../Lab11-Experiment1/\"+experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
