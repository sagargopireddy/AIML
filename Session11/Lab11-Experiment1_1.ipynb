{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations Of AIML\n",
    "## Session 11\n",
    "### Experiment 1.1: Initialization of Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will see 'Xavier', 'Zero', 'normal' and 'he' intialization of weights in a Lenet neural network.\n",
    "\n",
    "We will use CIFAR100 dataset. This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 50,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LeNet-5** a pioneering 7-level convolutional network by LeCun et al in 1998, that classifies digits, was applied by several banks to recognise hand-written numbers on checks (cheques) digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more convolutional layers, so this technique is constrained by the availability of computing resources.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/800/0*MU7G1aH1jw-6eFiD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xavier Initialization** (from the name *Xavier Glorot*)\n",
    "\n",
    "Assigning the network weights before we start training seems to be a random process, right? We don’t know anything about the data, so we are not sure how to assign the weights that would work in that particular case. One good way is to assign the weights from a Gaussian distribution. Obviously this distribution would have zero mean and some finite variance. Let’s consider a linear neuron:\n",
    "\n",
    "y = w1x1 + w2x2 + ... + wNxN + b\n",
    "\n",
    "With each passing layer, we want the variance to remain the same. **This helps us keep the signal from exploding to a high value or vanishing to zero** (Frequently asked interview question). In other words, we need to initialize the weights in such a way that the variance remains the same for x and y. This initialization process is known as Xavier initialization. You can read the original paper [here](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "# Importing config.py file\n",
    "import config as cf\n",
    "from utils import *\n",
    "## Importing python packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking for GPU instance\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#Intilizaing the accuracy value as zero\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transforms.Compose**  -  composes several transforms together.\n",
    "\n",
    "**transforms.RandomCrop** - crops the given Python Imaging Library (PIL) Image at a random location.\n",
    "\n",
    "**transforms.ToTensor** - converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "\n",
    "**transforms.Normalize** - normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. \n",
    "\n",
    "    input[channel] = (input[channel] - mean[channel]) / std[channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n[Phase 1] : Data Preparation')\n",
    "\n",
    "##dataset\n",
    "dataset = 'cifar100'\n",
    "\n",
    "# Preparing the dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cf.mean[dataset], cf.std[dataset]),\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing the test dataset\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cf.mean[dataset], cf.std[dataset]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and Loading the dataset\n",
    "\n",
    "The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of classes in the dataset\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Downloading the dataset\n",
    "trainset = torchvision.datasets.CIFAR100(root='/data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR100(root='/data', train=False, download=False, transform=transform_test)\n",
    "### Loading the dataset \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=8)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us define the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "        \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, init_mode='xavier'):  ### supports 'zero', 'normal', 'xavier', 'he' inits\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, num_classes)\n",
    "        \n",
    "        if init_mode == 'zero':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    m.weight.data.zero_()   ### fill tensor elements with zeros\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "        \n",
    "        if init_mode == 'normal':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    m.weight.data.normal_()   ### fill tensor elements with random numbers from normal distribution\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_()\n",
    "        \n",
    "        if init_mode == 'xavier':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    fan_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                    n = fan_in + fan_out\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, math.sqrt(2. / n))\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    size = m.weight.size()\n",
    "                    fan_out = size[0] # number of rows\n",
    "                    fan_in = size[1] # number of columns\n",
    "                    variance = math.sqrt(2.0/(fan_in+fan_out))\n",
    "                    m.weight.data.normal_(0.0, variance)\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, variance)\n",
    "        \n",
    "        if init_mode == 'he':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, math.sqrt(2. / n))\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    size = m.weight.size()\n",
    "                    fan_out = size[0] # number of rows\n",
    "                    fan_in = size[1] # number of columns\n",
    "                    variance = math.sqrt(2.0/(fan_in))\n",
    "                    m.weight.data.normal_(0.0, variance)\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, variance)\n",
    "                \n",
    "                \n",
    "    ## Forward Pass\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Visualisation at initialisation of parameters\n",
    "\n",
    "We will visualize only the first layer in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for init in ['zero', 'normal', 'xavier', 'he']:\n",
    "    print('Visualising ' + init)\n",
    "    net = LeNet(num_classes, init_mode=init)\n",
    "    for p in net.parameters():\n",
    "        for filter in range(p.size()[0]):\n",
    "            plt.imshow(p[filter].data.numpy().transpose(1,2,0), interpolation='bicubic')\n",
    "            plt.show()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Xavier init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calling the model\n",
    "net = LeNet(num_classes, init_mode='xavier')\n",
    "if use_cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Intiliazing the loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    ## Declaring the values\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ## Looping over the train data\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        ## Checking for Gpu instance \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        ## Optimizer\n",
    "        optimizer.zero_grad()\n",
    "        ## Converting inputs and targets into pytorch variables\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs)\n",
    "        ## Storing the size of output\n",
    "        size_ = outputs.size()\n",
    "        ## reducting the dimenssion of outputs\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        # Calculating the loss\n",
    "        loss = criterion(outputs_, targets)\n",
    "        ## backward pass\n",
    "        loss.backward()\n",
    "        ## optimizer\n",
    "        optimizer.step()\n",
    "        ## Calculating the training loss\n",
    "        train_loss += loss.data[0]\n",
    "        ## predicting the values\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        ## Storing the targets size\n",
    "        total += targets.size(0)\n",
    "        ## Calculating the corrected values\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        \n",
    "        ## Printing the data \n",
    "        if batch_idx%30 == 0 or batch_idx == len(trainloader)-1:\n",
    "            ## printing progress bar\n",
    "            progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    # Writing number of epoch, loss and accuracy into a file\n",
    "    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    ## Declaring the variables\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #Looping over test data\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # Checking for GPU instance\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # Converting inputs and targets into pytorch variable\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs)\n",
    "        ## Storing the size of output\n",
    "        size_ = outputs.size()\n",
    "        ## Reducing the dimenssion\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        # Calculating the loss\n",
    "        loss = criterion(outputs_, targets)\n",
    "        ## Calculating the test loss\n",
    "        test_loss += loss.data[0]\n",
    "        # Predicting the values\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        ## Storing the target size\n",
    "        total += targets.size(0)\n",
    "        # Calcuting the corrected values\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        ## printing the data\n",
    "        if batch_idx%30 == 0 or batch_idx == len(testloader)-1:\n",
    "            # Printing the progress bar\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    ## printing the loss    \n",
    "    print('val_loss: ',  test_loss/len(testloader), 'accuracy: ', 100.0*correct/total)\n",
    "    # Writing the number of epoch, loss and accuracy into a file\n",
    "    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    # Checking for best accuracy\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        # Checking for directory\n",
    "        if not os.path.isdir('../checkpoint'):\n",
    "            # Creating a directory\n",
    "            os.mkdir('../checkpoint')\n",
    "        # saving the checkpoint\n",
    "        torch.save(state, '../checkpoint_ckpt.t7')\n",
    "        ## storing best accuracy value\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment = 'xavier_init'\n",
    "# Creating a files in write mode\n",
    "train_loss_file = open(\"../Lab11-Experiment1/\"+experiment+\"train_loss.txt\", \"w\")\n",
    "val_loss_file = open(\"../Lab11-Experiment1/\"+experiment+\"val_loss.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Intiliazing the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0)\n",
    "## Training and Testing the model for 30 epochs\n",
    "for epoch in range(0, 30):\n",
    "    ## Traning the model\n",
    "    train(epoch)\n",
    "    ## Testing the model\n",
    "    test(epoch)\n",
    "    \n",
    "# Closing the files   \n",
    "train_loss_file.close()\n",
    "val_loss_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let us plot the training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_curves(\"../Lab11-Experiment1/\"+experiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in net.parameters():\n",
    "    for filter in range(p.size()[0]):\n",
    "        plt.imshow(p[filter].data.cpu().numpy().transpose(1,2,0), interpolation='bicubic', cmap='gray')\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. Compare training and test curves as well as the accuracy while using 'zero', 'normal', 'xavier', and 'he' inits and all other hyperparameters remaining same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework. \n",
    "a. It is hard to guess which filter is performing what task for this scenario. Try running this experiment on MNIST and visualize the filters.  \n",
    "b. Also, replace LeNet with AlexNet/VGG and visualize the filters again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
