{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations Of AIML\n",
    "## Session 11\n",
    "### Experiment 1.3: Learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is to tune the hyper parameter called learning rate and observe the changes in the output.\n",
    "\n",
    "We will use CIFAR100 dataset.This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 50,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "# Importing config.py file\n",
    "import config as cf\n",
    "from utils import *\n",
    "## Importing python packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for GPU instance\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#Intilizaing the accuracy value as zero\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n[Phase 1] : Data Preparation')\n",
    "\n",
    "##dataset\n",
    "dataset = 'cifar100'\n",
    "\n",
    "# Preparing the dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cf.mean[dataset], cf.std[dataset]),\n",
    "]) # meanstd transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the standard mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cf.mean[dataset], cf.std[dataset]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and Loading the dataset\n",
    "\n",
    "The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes in the dataset\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downloading the dataset\n",
    "trainset = torchvision.datasets.CIFAR100(root='/data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR100(root='/data', train=False, download=False, transform=transform_test)\n",
    "### Loading the dataset \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=8)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "        \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, init_mode='xavier'):  ### supports 'zero', 'normal', 'xavier', 'he' inits\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, num_classes)\n",
    "        \n",
    "        if init_mode == 'zero':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    m.weight.data.zero_()   ### fill tensor elements with zeros\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "        \n",
    "        if init_mode == 'normal':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    m.weight.data.normal_()   ### fill tensor elements with random numbers from normal distribution\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_()\n",
    "        \n",
    "        if init_mode == 'xavier':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    fan_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                    n = fan_in + fan_out\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, math.sqrt(2. / n))\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    size = m.weight.size()\n",
    "                    fan_out = size[0] # number of rows\n",
    "                    fan_in = size[1] # number of columns\n",
    "                    variance = math.sqrt(2.0/(fan_in+fan_out))\n",
    "                    m.weight.data.normal_(0.0, variance)\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, variance)\n",
    "        \n",
    "        if init_mode == 'he':\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, math.sqrt(2. / n))\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    size = m.weight.size()\n",
    "                    fan_out = size[0] # number of rows\n",
    "                    fan_in = size[1] # number of columns\n",
    "                    variance = math.sqrt(2.0/(fan_in))\n",
    "                    m.weight.data.normal_(0.0, variance)\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.normal_(0, variance)\n",
    "                \n",
    "                \n",
    "    ## Forward Pass\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Xavier init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling the model\n",
    "net = LeNet(num_classes, init_mode='xavier')\n",
    "# Checking for GPU instance\n",
    "if use_cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intiliazing the loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    #Declaring the variables\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Looping over train data\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Checking for GPU instance\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # Optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Converting targets and inputs into pytorch variables\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs)\n",
    "        # Storing the outputs size\n",
    "        size_ = outputs.size()\n",
    "        # Reducing the dimenssion\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        # Calculating the loss\n",
    "        loss = criterion(outputs_, targets)\n",
    "        # backward Pass\n",
    "        loss.backward()\n",
    "        # Optimizer Step\n",
    "        optimizer.step()\n",
    "        # Calculating the tarining loss\n",
    "        train_loss += loss.data[0]\n",
    "        # Predicting the values\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        # Storing the targets size\n",
    "        total += targets.size(0)\n",
    "        # Calculating the corrected values\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        # printing the data\n",
    "        if batch_idx%30 == 0 or batch_idx == len(trainloader)-1:\n",
    "            # printing the progress bar\n",
    "            progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    # Storing number of epoch,loss and accuracy in a file    \n",
    "    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    # Declaring the valules\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Looping over test data\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # Checking for GPU instance\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # Converting inputs and targets into pytorch variables\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        # Forward Pass\n",
    "        outputs = net(inputs)\n",
    "        # Storing the outputs size\n",
    "        size_ = outputs.size()\n",
    "        # Reducing the dimenssions\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        # Calculating the loss\n",
    "        loss = criterion(outputs_, targets)\n",
    "        # Calculating the test loss\n",
    "        test_loss += loss.data[0]\n",
    "        # Storing the predicted values\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        # Storing the targets size\n",
    "        total += targets.size(0)\n",
    "        # Caluculating the correct values\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        # Printing the data\n",
    "        if batch_idx%30 == 0 or batch_idx == len(testloader)-1:\n",
    "            # Printing the progress bar\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    # Printing the validation loss\n",
    "    print('val_loss: ',  test_loss/len(testloader), 'accuracy: ', 100.0*correct/total)\n",
    "    # Storing number of epoch,loss and accuracy in a file\n",
    "    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    # Checking for best accuracy\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        # Checking whether its a directory or not\n",
    "        if not os.path.isdir('../checkpoint'):\n",
    "            # Creating a directory\n",
    "            os.mkdir('../checkpoint')\n",
    "        # Saving the data\n",
    "        torch.save(state, '../checkpoint_ckpt.t7')\n",
    "        # Storing the best accuracy\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'lr_schedule'\n",
    "# Creating files in write mode\n",
    "train_loss_file = open(\"../Lab11-Experiment1/\"+experiment+\"train_loss.txt\", \"w\")\n",
    "val_loss_file = open(\"../Lab11-Experiment1/\"+experiment+\"val_loss.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training and testing the model for 60 epochs\n",
    "for epoch in range(0, 60):\n",
    "    if epoch == 50:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    if epoch == 30:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    if epoch == 0:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    # Training the model\n",
    "    train(epoch)\n",
    "    # Testing the model\n",
    "    test(epoch)\n",
    "# Closing the files \n",
    "train_loss_file.close()\n",
    "val_loss_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_curves(\"../Lab11-Experiment1/\"+experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Notice the slope of loss and accuracy for the epoch when learning rate is changed. What could be the reason of this jump?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2. Why do you think it would be better to start with a high learning rate and high momentum and subsequently decrease during the next epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
